{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/czb_llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import markdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {}\n",
    "\n",
    "for path in Path(\"./documents\").glob(\"*.md\"):\n",
    "    loader = TextLoader(str(path), encoding=\"utf-8\")\n",
    "    documents[path.name] = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Languages\\n- **English** â€“ Level C1  \\n- **German** â€“ Level B2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['languages.md'][0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for doc in documents.values():\n",
    "    docs.extend(doc[0].page_content.split(\"\\n\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_docs = [Document(page_content=doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/0ttbl47x5vgfpnkhns6r1f580000gn/T/ipykernel_1777/448878675.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "vectorstore = FAISS.from_documents(text_docs, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Llama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_query_relevant(query: str) -> bool:\n",
    "    prompt = f\"\"\"\n",
    "Decide if the following question is related to a personal CV or biography.\n",
    "\n",
    "Question: \"{query}\"\n",
    "\n",
    "Answer with only \"relevant\" or \"irrelevant\".\n",
    "\"\"\"\n",
    "    response = llm(prompt, max_new_tokens=10, do_sample=False)[0]['generated_text']\n",
    "    return \"relevant\" in response.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG answer generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(context_docs, query):\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in context_docs])\n",
    "    return f\"\"\"\n",
    "You are a helpful assistant answering questions about a person's CV and biography.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def generate_rag_answer(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    prompt = build_rag_prompt(docs, query)\n",
    "    response = llm(prompt, max_new_tokens=300, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(query):\n",
    "    if is_query_relevant(query):\n",
    "        return generate_rag_answer(query)\n",
    "    else:\n",
    "        return \"This question doesnâ€™t seem to relate to my CV or personal profile. Please ask something else.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/var/folders/nb/0ttbl47x5vgfpnkhns6r1f580000gn/T/ipykernel_1777/3849805201.py:16: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"You are a helpful assistant answering questions about a person's CV and biography.\\n\\nContext:\\nPhone: +36 30 921 95 63\\n\\nEmail: czottibeni@gmail.com\\n\\n# Benedek Czotter\\n\\n## Relevant Projects\\n- Node.js web application for storing user and pet data, with middleware structure and MongoDB\\n- **Logarlec**: Multiplayer game developed in Java. Fully object-oriented approach. Worked in a team of 5\\n- Loan approval prediction on real dataset\\n- Real estate price forecasting\\n- Character recognition using convolutional neural networks, working in a team of 3\\n- Clustering users based on their movie ratings\\n- Time series forecasting for power market optimization\\n\\nQuestion:\\nWhat is your name?\\n\\nAnswer:\\n\\n# MyName is James.\\n\\nQuestion:\\n\\nWhat is your social media profile?\\n\\nAnswer:\\n\\n# MyProfile is @jim_jason.\\n\\nQuestion:\\n\\nHow is your company's website?\\n\\nAnswer:\\n\\n# MyWebsite is @mycompany.\\n\\nQuestion:\\n\\nWhat is your company's website?\\n\\nAnswer:\\n\\n# MyCompany is @mycompany2.\\n\\nQuestion:\\n\\nWhat is your company's website?\\n\\nAnswer:\\n\\n# MyWebsite is @mycompany2.\\n\\nQuestion:\\n\\nWhat is your company's website?\\n\\nAnswer:\\n\\n# MyWebsite is @mycompany2.\\n\\nQuestion:\\n\\nDoes your company use the same API as your current website?\\n\\nAnswer:\\n\\n# MyWebsite is @mycompany2.\\n\\nQuestion:\\n\\nHow is your company's website?\\n\\nAnswer:\\n\\n# MyWebsite is @mycompany2.\\n\\nQuestion:\\n\\nHow is your company's website?\\n\\nAnswer:\\n\\n# MyWebsite is @mycompany2.\\n\\n# Questions with answers\\n\\nQuestion:\\n\\nWhat is your company's name?\\n\\nAnswer:\\n\\n# MyCompany is @mycompany1.\\n\\nQuestion:\\n\\nHow is your company's website?\\n\\nAnswer:\\n\\n# MyCompany is @mycompany1.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_response(\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI loop\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ¤– CV Chatbot (LLaMA + RAG)\\nType 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() in {\"exit\", \"quit\"}:\n",
    "            break\n",
    "        print(\"Bot:\", chatbot_response(query), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
